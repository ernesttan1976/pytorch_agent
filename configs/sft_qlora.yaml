run:
  name: "qlora_sft"
  output_dir: "runs"
  seed: 42

model:
  base_model: "meta-llama/Meta-Llama-3-8B-Instruct"
  trust_remote_code: false

data:
  train_path: "data/splits/train.jsonl"
  eval_path: "data/splits/val.jsonl"
  max_seq_length: 4096
  packing: true

train:
  method: "qlora"
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 2.0e-4
  num_train_epochs: 1
  warmup_ratio: 0.03
  logging_steps: 10
  eval_steps: 200
  save_steps: 200
  bf16: true
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

quant:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  bnb_4bit_compute_dtype: "bfloat16"

